{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1+cu117'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.24.3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "np.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. **Buidling blocks**\n",
    "+ scalar = 0-dimension tensor\n",
    "+ vector = 1-dimension tensor\n",
    "+ **matrix** vs **tensor**:  \n",
    "    **matrix** = 2-dimension tensor\n",
    "=> **Dimension** is how many **nested layers** of **lists** are there in a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n",
      "scalar.ndim = 0\n",
      "tensor_high.ndim = 3\n"
     ]
    }
   ],
   "source": [
    "# scalar is 0-dimension tensor, 1-dimension vector/matrix\n",
    "tensor_high = torch.tensor([\n",
    "        [\n",
    "            [1,1,2,3],\n",
    "            [1,2,2,4],\n",
    "        ],\n",
    "        [\n",
    "            [1,1,2,3],\n",
    "            [1,2,2,4],\n",
    "        ],\n",
    "        [\n",
    "            [1,1,2,3],\n",
    "            [1,2,2,4],\n",
    "        ]\n",
    "    ])\n",
    "scalar = torch.tensor(1)\n",
    "print(scalar)\n",
    "print(f\"{scalar.ndim = }\")\n",
    "print(f\"{tensor_high.ndim = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scalar.item() = 1\n",
      "only one element tensors can be converted to Python scalars\n"
     ]
    }
   ],
   "source": [
    "# get the value inside a 0-dimension (or a scalar)\n",
    "# .item() can only work with scalar\n",
    "print(f\"{scalar.item() = }\")\n",
    "try:\n",
    "    print(f\"{tensor_high.item() = }\") # will throw error\n",
    "except Exception as err:\n",
    "    print(err)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 2, 3],\n",
       "         [1, 2, 3],\n",
       "         [1, 2, 3]],\n",
       "\n",
       "        [[1, 2, 3],\n",
       "         [1, 2, 3],\n",
       "         [1, 2, 0]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# every element of a tensor has to have equal length\n",
    "torch.tensor(\n",
    "    [\n",
    "    [[1,2,3],[1,2,3],[1,2,3]],\n",
    "    [[1,2,3],[1,2,3],[1,2,0]]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST_TENSOR.shape = torch.Size([3, 2])\n",
      "\n",
      "np.array(TEST_TENSOR).shape = (3, 2)\n",
      "\n",
      "TEST_TENSOR.ndim = 2\n",
      "\n",
      "np.array(TEST_TENSOR).ndim = 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TEST_TENSOR = torch.tensor([[2,1],[4,2],[1,2]])\n",
    "\n",
    "'''\n",
    "Dimensions in numpy array (or tensors) are number of \n",
    "nested elements, or number of nested sets of values:\n",
    "-> to get the shape of a tensor/array: how the elements are arranged\n",
    "    + np.array.shape == torch.tensor.shape\n",
    "-> to get the dimensions a tensor/array\n",
    "    + np.array.ndim == torch.tensor.ndim\n",
    "'''\n",
    "print(f\"{TEST_TENSOR.shape = }\\n\")\n",
    "print(f\"{np.array(TEST_TENSOR).shape = }\\n\")\n",
    "print(f\"{TEST_TENSOR.ndim = }\\n\")\n",
    "print(f\"{np.array(TEST_TENSOR).ndim = }\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TENSOR = torch.tensor([[[1, 2, 3],\n",
    "                        [3, 6, 9],\n",
    "                        [2, 4, 5]],\n",
    "                        [[1, 2, 3],\n",
    "                        [3, 6, 9],\n",
    "                        [2, 4, 5]]])\n",
    "MATRIX = torch.tensor([[1,2],[4,5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# TENSOR and MATRIX are often uppercase and using interchagebly\n",
    "# MATRIX is 2 dimensional, -> 2-dimension tensor\n",
    "print(TENSOR.ndim)\n",
    "print(MATRIX.ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. **Gain Momentum**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Dummy tensor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dummy tensor\n",
    "# the model/sofware will fill in/update that tensor later\n",
    "RANDOM_TENSORS = torch.rand(size = (3,2,4))\n",
    "ZEROS_TENSORS = torch.zeros(size = (3,2,4))\n",
    "ONES_TENSORS = torch.ones(size = (3,2,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.4984, 0.4401, 0.2846, 0.9075],\n",
       "          [0.1119, 0.9037, 0.7529, 0.7300]],\n",
       " \n",
       "         [[0.8648, 0.5590, 0.6621, 0.5348],\n",
       "          [0.3053, 0.8256, 0.7486, 0.9975]],\n",
       " \n",
       "         [[0.5802, 0.3854, 0.5247, 0.3909],\n",
       "          [0.0185, 0.8985, 0.7334, 0.0821]]]),\n",
       " tensor([[[0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.]]]),\n",
       " tensor([[[1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.]],\n",
       " \n",
       "         [[1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.]],\n",
       " \n",
       "         [[1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.]]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RANDOM_TENSORS,ZEROS_TENSORS,ONES_TENSORS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Generate range (1-d tensor, vector)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_to_one = torch.arange(0,1,.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(type(zero_to_one))\n",
    "print(zero_to_one.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create tensors of zeros/ones with the same shape of another tensor\n",
    "zeros_like = torch.ones_like(input=zero_to_one)\n",
    "ones_like = torch.zeros_like(input=zero_to_one)\n",
    "zeros_like,ones_like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Datatypes**\n",
    "+ some are better computed on **CPU**, and others, **GPU**\n",
    "+ default is torch.**float32** (or torch.**float**):\n",
    "    + torch.**float16** (torch.**half**)\n",
    "    + torch.**float64** (torch.**double**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/durian/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returns the number of GPUs available\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3]), torch.float32, device(type='cpu'))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Default datatype for tensors is float32\n",
    "# Default device is cpu\n",
    "float_32_tensor = torch.tensor([3.0, 6.0, 9.0])\n",
    "\n",
    "float_32_tensor.shape, float_32_tensor.dtype, float_32_tensor.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No CUDA GPUs are available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/durian/learning/PyTorch_24hrs/0/00_fundamentals.ipynb Cell 21\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/durian/learning/PyTorch_24hrs/0/00_fundamentals.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# assign a floating float16 tensor to gpu\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/durian/learning/PyTorch_24hrs/0/00_fundamentals.ipynb#X26sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m float_16_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor([\u001b[39m3.0\u001b[39;49m, \u001b[39m6.0\u001b[39;49m, \u001b[39m9.0\u001b[39;49m],\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/durian/learning/PyTorch_24hrs/0/00_fundamentals.ipynb#X26sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m                                dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat16,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/durian/learning/PyTorch_24hrs/0/00_fundamentals.ipynb#X26sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                                device\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m\"\u001b[39;49m) \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/durian/learning/PyTorch_24hrs/0/00_fundamentals.ipynb#X26sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m float_16_tensor\u001b[39m.\u001b[39mtype(),float_16_tensor\u001b[39m.\u001b[39mdevice\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:229\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mCUDA_MODULE_LOADING\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39menviron:\n\u001b[1;32m    228\u001b[0m     os\u001b[39m.\u001b[39menviron[\u001b[39m'\u001b[39m\u001b[39mCUDA_MODULE_LOADING\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mLAZY\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 229\u001b[0m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_cuda_init()\n\u001b[1;32m    230\u001b[0m \u001b[39m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[39m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[39m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    233\u001b[0m _tls\u001b[39m.\u001b[39mis_initializing \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No CUDA GPUs are available"
     ]
    }
   ],
   "source": [
    "# assign a floating float16 tensor to gpu\n",
    "float_16_tensor = torch.tensor([3.0, 6.0, 9.0],\n",
    "                               dtype=torch.float16,\n",
    "                               device=\"cuda\") \n",
    "\n",
    "float_16_tensor.type(),float_16_tensor.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ **When ever** there's a problem, it is usually related to one of these three:\n",
    "    + **Where** is my tensor **stored**\n",
    "    + What is the **shape** of my tensor (most of the time)\n",
    "    + What is the **type** of *my tensor*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. **Operations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = torch.tensor([1,2,3])\n",
    "print(vector + 10)\n",
    "print(vector - 10)\n",
    "print(vector * vector)\n",
    "print(vector / 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.Multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1. **Matrix** multiplication => create **dot product**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_TENSOR_1 = torch.rand(size = (2,3))\n",
    "test_TENSOR_2 = torch.rand(size = (3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can only be done with the number of rows equal to number of columns\n",
    "print(test_TENSOR_1 @ test_TENSOR_2) # using @ \n",
    "# using torch.matmul()\n",
    "# torch.mm() has the same effect\n",
    "print(torch.matmul(test_TENSOR_1,test_TENSOR_2)) \n",
    "\n",
    "try:\n",
    "    test_TENSOR_2 @ test_TENSOR_1\n",
    "except Exception as e:\n",
    "    print(str(e)+\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2. **Element-wise** multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    test_TENSOR_1 * test_TENSOR_2\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# multication among individual values of 2 matrix\n",
    "# => 2 matrices have to have to same size \n",
    "\n",
    "test_TENSOR_1 * torch.tensor([[1,2,3],[1,4,1]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. **Tensor aggregations** (min, max, mean, median, sum, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1. **Basics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_MATRIX = torch.tensor([\n",
    "                [1,2,1],\n",
    "                [1,54,4],\n",
    "                [1,9,6]\n",
    "                ],dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.mean(test_MATRIX),test_MATRIX.mean())\n",
    "print(torch.sum(test_MATRIX),test_MATRIX.sum())\n",
    "print(torch.max(test_MATRIX),test_MATRIX.max())\n",
    "print(torch.min(test_MATRIX),test_MATRIX.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2. Find **positional min and max**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    test_MATRIX.argmax(dim=1,keepdim=True),\n",
    "    test_MATRIX.argmax(dim=0,keepdim=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_MATRIX[0,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. **Changing the shape of the tensor**\n",
    "+ **Reshapping** - reshape input into a redefined shape (**change memory**)\n",
    "+ **View** - new tensor is **NOT** created => using the **same memory slots** as the input tensor\n",
    "+ **Stacking** - merge multiple tensors (**horizontally** or **vertically**)\n",
    "+ **Squeeze** - removes all 1-d's from a tensor ???\n",
    "+ **Unsqueeze** - add a 1-d to a target tensor\n",
    "+ **Permute** - return a **view** of the input tensor, but **dimensions permuted (swappedd)** in a particular way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. **Reshaping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(0,100,1)\n",
    "x,x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESHAPE, add an extra dimension\n",
    "x_reshape = x.reshape(1,100)\n",
    "x_reshape.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the shape has to fit all the original value\n",
    "try:\n",
    "    x_reshape = x.reshape(1,99)\n",
    "except Exception as e:\n",
    "    print(str(e))\n",
    "\n",
    "# shape of 10,2,5\n",
    "# 3 tensors\n",
    "x_reshape = x.reshape(10,2,5)\n",
    "print(f\"{x_reshape.shape = }\")\n",
    "\n",
    "print(f\"{x_reshape.ndim = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. **View**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just like the reshaping, but there's not memmory wasted\n",
    "x_view = x.view(2,2,25)\n",
    "print(f\"{x_view.shape = }\")\n",
    "print(f\"{x_view.ndim = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing a view with change the input tensor\n",
    "## at the index 0 of first tensor, 0 of second tensor\n",
    "## change the value of the 1 and 2-indexed scalar to 99 and 99\n",
    "x_view[0,0,1:3] = torch.tensor([99,99])\n",
    "\n",
    "# check the input tensor\n",
    "x[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. **Stack**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new dimension\n",
    "# change the shape\n",
    "print(f\"{torch.stack([x,x],dim=0).shape = }\")\n",
    "print(f\"{torch.stack([x,x],dim=1).shape = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the tensor can only be stacked on existing dimension\n",
    "## In this example, x_reshape has 3 dimensions\n",
    "## => can be stacked in 4 different dimension\n",
    "print(f\"{torch.stack([x_reshape,x_reshape],dim=0).shape = }\")\n",
    "print(f\"{torch.stack([x_reshape,x_reshape],dim=1).shape = }\")\n",
    "print(f\"{torch.stack([x_reshape,x_reshape],dim=2).shape = }\")\n",
    "print(f\"{torch.stack([x_reshape,x_reshape],dim=3).shape = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. **Squeezing** and **Permuting** tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# squeezing, removing 1-d's\n",
    "x_zeros = torch.zeros(size=(1,21,3,4))\n",
    "print(f\"Original shape of the tensor: {x_zeros.shape}\")\n",
    "print(f\"Squeeze at the 1st dimension {x_zeros.squeeze(0).shape}\")\n",
    "print(f\"Squeeze at the 2nd dimension {x_zeros.squeeze(1).shape}\")\n",
    "print(f\"Squeeze at the 3rd dimension {x_zeros.squeeze(2).shape}\")\n",
    "print(f\"Squeeze at all dimensions {x_zeros.squeeze().shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add dimension, unsqueez()\n",
    "x_zeros.unsqueeze(dim=3).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# permute, rearange the dimension\n",
    "# this is also a view\n",
    "x_zeros.permute(1,0,3,2).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. **Indexing** (similar to *NumPy*)\n",
    "+ when turning from **numpy.array** to **torch.tensor** (and vice versa), the **datatype** prevail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(0.0,100.,5).reshape(5,2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[:,0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.type(torch.int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. **Reproducibility**\n",
    "+ Using random seeds to generate **identical random data**\n",
    "+ Reproduce the code in others' machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_TENSOR_a = torch.rand(3,4)\n",
    "random_TENSOR_b = torch.rand(3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_TENSOR_a == random_TENSOR_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed = 666)\n",
    "random_TENSOR_a = torch.rand(3,4)\n",
    "\n",
    "# has to reset the seed every time torch.rand() is called\n",
    "torch.manual_seed(seed = 666)\n",
    "random_TENSOR_b = torch.rand(3,4)\n",
    "\n",
    "random_TENSOR_a == random_TENSOR_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# !!!7. **Running tensors on GPUs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1. Config GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "# check the CUDA satus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if torch can connect to a GPU\n",
    "torch.cuda.is_available()\n",
    "\n",
    "# It's a good practice to write \"device agnostic code\"\n",
    "# The code can now ultilizingly run on cpu or gpu\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1. Transfer tensor to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of GPU\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_TENSOR = torch.tensor([1,2,3])\n",
    "print(test_TENSOR.device)\n",
    "print(test_TENSOR.to(device).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TENSOR_GPU = test_TENSOR.to(device)\n",
    "# TENSOR_GPU = test_TENSOR.cuda() working the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    TENSOR_GPU.numpy()\n",
    "except Exception as e:\n",
    "    print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy tensor back to the cpu \n",
    "TENSOR_backFromGPU = test_TENSOR.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TENSOR_backFromGPU.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('data_science')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a10e4b5ad14839566a2f735df5e9b3727c3adb97c6db97ba3b9982fff39dfe07"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
