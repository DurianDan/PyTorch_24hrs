{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1+cu117'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.24.3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "np.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. **Buidling blocks**\n",
    "+ scalar = 0-dimension tensor\n",
    "+ vector = 1-dimension tensor\n",
    "+ **matrix** vs **tensor**:  \n",
    "    **matrix** = 2-dimension tensor\n",
    "=> **Dimension** is how many **nested layers** of **lists** are there in a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n",
      "scalar.ndim = 0\n",
      "tensor_high.ndim = 3\n"
     ]
    }
   ],
   "source": [
    "# scalar is 0-dimension tensor, 1-dimension vector/matrix\n",
    "tensor_high = torch.tensor([\n",
    "        [\n",
    "            [1,1,2,3],\n",
    "            [1,2,2,4],\n",
    "        ],\n",
    "        [\n",
    "            [1,1,2,3],\n",
    "            [1,2,2,4],\n",
    "        ],\n",
    "        [\n",
    "            [1,1,2,3],\n",
    "            [1,2,2,4],\n",
    "        ]\n",
    "    ])\n",
    "scalar = torch.tensor(1)\n",
    "print(scalar)\n",
    "print(f\"{scalar.ndim = }\")\n",
    "print(f\"{tensor_high.ndim = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scalar.item() = 1\n",
      "only one element tensors can be converted to Python scalars\n"
     ]
    }
   ],
   "source": [
    "# get the value inside a 0-dimension (or a scalar)\n",
    "# .item() can only work with scalar\n",
    "print(f\"{scalar.item() = }\")\n",
    "try:\n",
    "    print(f\"{tensor_high.item() = }\") # will throw error\n",
    "except Exception as err:\n",
    "    print(err)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 2, 3],\n",
       "         [1, 2, 3],\n",
       "         [1, 2, 3]],\n",
       "\n",
       "        [[1, 2, 3],\n",
       "         [1, 2, 3],\n",
       "         [1, 2, 0]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# every element of a tensor has to have equal length\n",
    "torch.tensor(\n",
    "    [\n",
    "    [[1,2,3],[1,2,3],[1,2,3]],\n",
    "    [[1,2,3],[1,2,3],[1,2,0]]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST_TENSOR.shape = torch.Size([3, 2])\n",
      "\n",
      "np.array(TEST_TENSOR).shape = (3, 2)\n",
      "\n",
      "TEST_TENSOR.ndim = 2\n",
      "\n",
      "np.array(TEST_TENSOR).ndim = 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TEST_TENSOR = torch.tensor([[2,1],[4,2],[1,2]])\n",
    "\n",
    "'''\n",
    "Dimensions in numpy array (or tensors) are number of \n",
    "nested elements, or number of nested sets of values:\n",
    "-> to get the shape of a tensor/array: how the elements are arranged\n",
    "    + np.array.shape == torch.tensor.shape\n",
    "-> to get the dimensions a tensor/array\n",
    "    + np.array.ndim == torch.tensor.ndim\n",
    "'''\n",
    "print(f\"{TEST_TENSOR.shape = }\\n\")\n",
    "print(f\"{np.array(TEST_TENSOR).shape = }\\n\")\n",
    "print(f\"{TEST_TENSOR.ndim = }\\n\")\n",
    "print(f\"{np.array(TEST_TENSOR).ndim = }\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TENSOR = torch.tensor([[[1, 2, 3],\n",
    "                        [3, 6, 9],\n",
    "                        [2, 4, 5]],\n",
    "                        [[1, 2, 3],\n",
    "                        [3, 6, 9],\n",
    "                        [2, 4, 5]]])\n",
    "MATRIX = torch.tensor([[1,2],[4,5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# TENSOR and MATRIX are often uppercase and using interchagebly\n",
    "# MATRIX is 2 dimensional, -> 2-dimension tensor\n",
    "print(TENSOR.ndim)\n",
    "print(MATRIX.ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. **Gain Momentum**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Dummy tensor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dummy tensor\n",
    "# the model/sofware will fill in/update that tensor later\n",
    "RANDOM_TENSORS = torch.rand(size = (3,2,4))\n",
    "ZEROS_TENSORS = torch.zeros(size = (3,2,4))\n",
    "ONES_TENSORS = torch.ones(size = (3,2,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.6790, 0.0994, 0.8359, 0.1178],\n",
       "          [0.0443, 0.0352, 0.1131, 0.7228]],\n",
       " \n",
       "         [[0.8905, 0.3600, 0.4909, 0.0643],\n",
       "          [0.9698, 0.7325, 0.3516, 0.4891]],\n",
       " \n",
       "         [[0.0977, 0.4954, 0.8065, 0.7519],\n",
       "          [0.2045, 0.3041, 0.5311, 0.1946]]]),\n",
       " tensor([[[0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.]]]),\n",
       " tensor([[[1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.]],\n",
       " \n",
       "         [[1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.]],\n",
       " \n",
       "         [[1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.]]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RANDOM_TENSORS,ZEROS_TENSORS,ONES_TENSORS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Generate range (1-d tensor, vector)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_to_one = torch.arange(0,1,.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(type(zero_to_one))\n",
    "print(zero_to_one.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create tensors of zeros/ones with the same shape of another tensor\n",
    "zeros_like = torch.ones_like(input=zero_to_one)\n",
    "ones_like = torch.zeros_like(input=zero_to_one)\n",
    "zeros_like,ones_like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Datatypes**\n",
    "+ some are better computed on **CPU**, and others, **GPU**\n",
    "+ default is torch.**float32** (or torch.**float**):\n",
    "    + torch.**float16** (torch.**half**)\n",
    "    + torch.**float64** (torch.**double**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/durian/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returns the number of GPUs available\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3]), torch.float32, device(type='cpu'))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Default datatype for tensors is float32\n",
    "# Default device is cpu\n",
    "float_32_tensor = torch.tensor([3.0, 6.0, 9.0])\n",
    "\n",
    "float_32_tensor.shape, float_32_tensor.dtype, float_32_tensor.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No CUDA GPUs are available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/durian/learning/PyTorch_24hrs/0/00_fundamentals.ipynb Cell 21\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/durian/learning/PyTorch_24hrs/0/00_fundamentals.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# assign a floating float16 tensor to gpu\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/durian/learning/PyTorch_24hrs/0/00_fundamentals.ipynb#X26sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m float_16_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor([\u001b[39m3.0\u001b[39;49m, \u001b[39m6.0\u001b[39;49m, \u001b[39m9.0\u001b[39;49m],\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/durian/learning/PyTorch_24hrs/0/00_fundamentals.ipynb#X26sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m                                dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat16,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/durian/learning/PyTorch_24hrs/0/00_fundamentals.ipynb#X26sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                                device\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m\"\u001b[39;49m) \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/durian/learning/PyTorch_24hrs/0/00_fundamentals.ipynb#X26sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m float_16_tensor\u001b[39m.\u001b[39mtype(),float_16_tensor\u001b[39m.\u001b[39mdevice\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:229\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mCUDA_MODULE_LOADING\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39menviron:\n\u001b[1;32m    228\u001b[0m     os\u001b[39m.\u001b[39menviron[\u001b[39m'\u001b[39m\u001b[39mCUDA_MODULE_LOADING\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mLAZY\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 229\u001b[0m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_cuda_init()\n\u001b[1;32m    230\u001b[0m \u001b[39m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[39m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[39m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    233\u001b[0m _tls\u001b[39m.\u001b[39mis_initializing \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No CUDA GPUs are available"
     ]
    }
   ],
   "source": [
    "# assign a floating float16 tensor to gpu\n",
    "float_16_tensor = torch.tensor([3.0, 6.0, 9.0],\n",
    "                               dtype=torch.float16,\n",
    "                               device=\"cuda\") \n",
    "\n",
    "float_16_tensor.type(),float_16_tensor.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ **When ever** there's a problem, it is usually related to one of these three:\n",
    "    + **Where** is my tensor **stored**\n",
    "    + What is the **shape** of my tensor (most of the time)\n",
    "    + What is the **type** of *my tensor*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. **Operations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([11, 12, 13])\n",
      "tensor([-9, -8, -7])\n",
      "tensor([1, 4, 9])\n",
      "tensor([0.1000, 0.2000, 0.3000])\n"
     ]
    }
   ],
   "source": [
    "vector = torch.tensor([1,2,3])\n",
    "print(vector + 10)\n",
    "print(vector - 10)\n",
    "print(vector * vector)\n",
    "print(vector / 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.Multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1. **Matrix** multiplication => create **dot product**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_TENSOR_1 = torch.rand(size = (2,3))\n",
    "test_TENSOR_2 = torch.rand(size = (3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8784, 0.9100, 1.1174, 0.8734],\n",
      "        [0.8957, 0.7095, 1.0524, 1.0648]])\n",
      "tensor([[0.8784, 0.9100, 1.1174, 0.8734],\n",
      "        [0.8957, 0.7095, 1.0524, 1.0648]])\n",
      "mat1 and mat2 shapes cannot be multiplied (3x4 and 2x3)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# can only be done with the number of rows equal to number of columns\n",
    "print(test_TENSOR_1 @ test_TENSOR_2) # using @ \n",
    "# using torch.matmul()\n",
    "# torch.mm() has the same effect\n",
    "print(torch.matmul(test_TENSOR_1,test_TENSOR_2)) \n",
    "\n",
    "try:\n",
    "    test_TENSOR_2 @ test_TENSOR_1\n",
    "except Exception as e:\n",
    "    print(str(e)+\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2. **Element-wise** multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of tensor a (3) must match the size of tensor b (4) at non-singleton dimension 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.6394, 1.3252, 1.6205],\n",
       "        [0.9029, 1.1835, 0.9868]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    test_TENSOR_1 * test_TENSOR_2\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# multication among individual values of 2 matrix\n",
    "# => 2 matrices have to have to same size \n",
    "\n",
    "test_TENSOR_1 * torch.tensor([[1,2,3],[1,4,1]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. **Tensor aggregations** (min, max, mean, median, sum, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1. **Basics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_MATRIX = torch.tensor([\n",
    "                [1,2,1],\n",
    "                [1,54,4],\n",
    "                [1,9,6]\n",
    "                ],dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.7778) tensor(8.7778)\n",
      "tensor(79.) tensor(79.)\n",
      "tensor(54.) tensor(54.)\n",
      "tensor(1.) tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "print(torch.mean(test_MATRIX),test_MATRIX.mean())\n",
    "print(torch.sum(test_MATRIX),test_MATRIX.sum())\n",
    "print(torch.max(test_MATRIX),test_MATRIX.max())\n",
    "print(torch.min(test_MATRIX),test_MATRIX.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(63.)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_MATRIX[:2].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2. Find **positional min and max**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1],\n",
      "        [1],\n",
      "        [1]]) tensor([[0, 1, 2]])\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    test_MATRIX.argmax(dim=1,keepdim=True),\n",
    "    test_MATRIX.argmax(dim=0,keepdim=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_MATRIX[0,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. **Changing the shape of the tensor**\n",
    "+ **Reshapping** - reshape input into a redefined shape (**change memory**)\n",
    "+ **View** - new tensor is **NOT** created => using the **same memory slots** as the input tensor\n",
    "+ **Stacking** - merge multiple tensors (**horizontally** or **vertically**)\n",
    "+ **Squeeze** - removes all 1-d's from a tensor ???\n",
    "+ **Unsqueeze** - add a 1-d to a target tensor\n",
    "+ **Permute** - return a **view** of the input tensor, but **dimensions permuted (swappedd)** in a particular way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. **Reshaping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VECTOR_TO_SHAPE = torch.arange(1, 5)\n",
    "VECTOR_TO_SHAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [2],\n",
       "        [3],\n",
       "        [4]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VECTOR_TO_SHAPE.reshape([4,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. **View**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VECTOR_TO_SHAPE.view([2,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. **Stack**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1],\n",
       "        [2, 2],\n",
       "        [3, 3],\n",
       "        [4, 4]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([\n",
    "    VECTOR_TO_SHAPE,\n",
    "    VECTOR_TO_SHAPE,\n",
    "],\n",
    "dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([\n",
    "    torch.ones(3),\n",
    "    torch.ones(3),\n",
    "], dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. **Squeezing** and **Unsqueezing** tensors\n",
    "- It's like *dimensionality reduction* in matrixes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.1. **Squeezing**\n",
    "> It's **merging dimensions** that have only **1 element**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TENSOR_TO_SQUEEZE = tensor([[[[1, 2, 2, 2, 2]],\n",
      "\n",
      "         [[1, 2, 2, 2, 2]]]])\n",
      "TENSOR_TO_SQUEEZE.shape = torch.Size([1, 2, 1, 5])\n"
     ]
    }
   ],
   "source": [
    "TENSOR_TO_SQUEEZE = torch.tensor([[\n",
    "    [\n",
    "        [1 , 2, 2, 2, 2],\n",
    "    ],\n",
    "    [\n",
    "        [1 , 2, 2, 2, 2],\n",
    "    ],\n",
    "]])\n",
    "print(f\"{TENSOR_TO_SQUEEZE = }\")\n",
    "print(f\"{TENSOR_TO_SQUEEZE.shape = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TENSOR_TO_SQUEEZE.squeeze() = tensor([[1, 2, 2, 2, 2],\n",
      "        [1, 2, 2, 2, 2]])\n",
      "TENSOR_TO_SQUEEZE.squeeze().shape = torch.Size([2, 5])\n"
     ]
    }
   ],
   "source": [
    "print(f\"{TENSOR_TO_SQUEEZE.squeeze() = }\")\n",
    "print(f\"{TENSOR_TO_SQUEEZE.squeeze().shape = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.2. **Unsqueezing**\n",
    "> It's adding **specified dimensions** that have only **1 element**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2]],\n",
       "\n",
       "        [[1],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2]]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TENSOR_TO_SQUEEZE.squeeze().unsqueeze(dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 2, 2, 2, 2],\n",
       "         [1, 2, 2, 2, 2]]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TENSOR_TO_SQUEEZE.squeeze().unsqueeze(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### => Wrapping **elements** inside of **specified dimension** with **additional brackets**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5. **Permuting** tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. **Indexing** (similar to *NumPy*)\n",
    "+ when turning from **numpy.array** to **torch.tensor** (and vice versa), the **datatype** prevail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(0.0,100.,5).reshape(5,2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5., 25., 45., 65., 85.])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  5],\n",
       "         [10, 15]],\n",
       "\n",
       "        [[20, 25],\n",
       "         [30, 35]],\n",
       "\n",
       "        [[40, 45],\n",
       "         [50, 55]],\n",
       "\n",
       "        [[60, 65],\n",
       "         [70, 75]],\n",
       "\n",
       "        [[80, 85],\n",
       "         [90, 95]]], dtype=torch.int8)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.type(torch.int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. **Reproducibility**\n",
    "+ Using random seeds to generate **identical random data**\n",
    "+ Reproduce the code in others' machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_TENSOR_a = torch.rand(3,4)\n",
    "random_TENSOR_b = torch.rand(3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False],\n",
       "        [False, False, False, False],\n",
       "        [False, False, False, False]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_TENSOR_a == random_TENSOR_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True],\n",
       "        [True, True, True, True],\n",
       "        [True, True, True, True]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(seed = 666)\n",
    "random_TENSOR_a = torch.rand(3,4)\n",
    "\n",
    "# has to reset the seed every time torch.rand() is called\n",
    "torch.manual_seed(seed = 666)\n",
    "random_TENSOR_b = torch.rand(3,4)\n",
    "\n",
    "random_TENSOR_a == random_TENSOR_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# !!!7. **Running tensors on GPUs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1. Config GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "# check the CUDA satus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if torch can connect to a GPU\n",
    "torch.cuda.is_available()\n",
    "\n",
    "# It's a good practice to write \"device agnostic code\"\n",
    "# The code can now ultilizingly run on cpu or gpu\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1. Transfer tensor to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the number of GPU\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "test_TENSOR = torch.tensor([1,2,3])\n",
    "print(test_TENSOR.device)\n",
    "print(test_TENSOR.to(device).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "TENSOR_GPU = test_TENSOR.to(device)\n",
    "# TENSOR_GPU = test_TENSOR.cuda() working the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    TENSOR_GPU.numpy()\n",
    "except Exception as e:\n",
    "    print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy tensor back to the cpu \n",
    "TENSOR_backFromGPU = test_TENSOR.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TENSOR_backFromGPU.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('data_science')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a10e4b5ad14839566a2f735df5e9b3727c3adb97c6db97ba3b9982fff39dfe07"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
